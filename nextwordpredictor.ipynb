{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "height": 1000
        },
        "id": "yMx5Ymff_lDE",
        "outputId": "fa61aadd-125e-4e1b-c5fd-e1a9e3a21444"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1b8406a1-049c-4ba7-8681-bf1977ae6380\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1b8406a1-049c-4ba7-8681-bf1977ae6380\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 1661-0.txt to 1661-0.txt\n",
            "8624\n",
            "The Length of sequences are:  108955\n",
            "Data:  [[ 142 4680    1]\n",
            " [4680    1  986]\n",
            " [   1  986    5]\n",
            " [ 986    5  125]\n",
            " [   5  125   33]\n",
            " [ 125   33   46]\n",
            " [  33   46  556]\n",
            " [  46  556 2164]\n",
            " [ 556 2164 2165]\n",
            " [2164 2165   27]]\n",
            "Response:  [ 986    5  125   33   46  556 2164 2165   27  987]\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 3, 10)             86240     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 3, 1000)           4044000   \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 1000)              8004000   \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1000)              1001000   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 8624)              8632624   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 21,767,864\n",
            "Trainable params: 21,767,864\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/70\n",
            "1700/1703 [============================>.] - ETA: 0s - loss: 6.4089\n",
            "Epoch 00001: loss improved from inf to 6.40869, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 35s 17ms/step - loss: 6.4087\n",
            "Epoch 2/70\n",
            "1702/1703 [============================>.] - ETA: 0s - loss: 5.8056\n",
            "Epoch 00002: loss improved from 6.40869 to 5.80559, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 30s 17ms/step - loss: 5.8056\n",
            "Epoch 3/70\n",
            "1701/1703 [============================>.] - ETA: 0s - loss: 5.4677\n",
            "Epoch 00003: loss improved from 5.80559 to 5.46741, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 30s 18ms/step - loss: 5.4674\n",
            "Epoch 4/70\n",
            "1701/1703 [============================>.] - ETA: 0s - loss: 5.2054\n",
            "Epoch 00004: loss improved from 5.46741 to 5.20554, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 30s 18ms/step - loss: 5.2055\n",
            "Epoch 5/70\n",
            "1701/1703 [============================>.] - ETA: 0s - loss: 4.9730\n",
            "Epoch 00005: loss improved from 5.20554 to 4.97287, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 30s 18ms/step - loss: 4.9729\n",
            "Epoch 6/70\n",
            "1702/1703 [============================>.] - ETA: 0s - loss: 4.7475\n",
            "Epoch 00006: loss improved from 4.97287 to 4.74757, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 30s 18ms/step - loss: 4.7476\n",
            "Epoch 7/70\n",
            "1702/1703 [============================>.] - ETA: 0s - loss: 4.5241\n",
            "Epoch 00007: loss improved from 4.74757 to 4.52400, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 30s 18ms/step - loss: 4.5240\n",
            "Epoch 8/70\n",
            "1702/1703 [============================>.] - ETA: 0s - loss: 4.2969\n",
            "Epoch 00008: loss improved from 4.52400 to 4.29677, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 30s 18ms/step - loss: 4.2968\n",
            "Epoch 9/70\n",
            "1701/1703 [============================>.] - ETA: 0s - loss: 4.0669\n",
            "Epoch 00009: loss improved from 4.29677 to 4.06677, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 30s 17ms/step - loss: 4.0668\n",
            "Epoch 10/70\n",
            "1701/1703 [============================>.] - ETA: 0s - loss: 3.8292\n",
            "Epoch 00010: loss improved from 4.06677 to 3.82918, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 30s 18ms/step - loss: 3.8292\n",
            "Epoch 11/70\n",
            "1703/1703 [==============================] - ETA: 0s - loss: 3.5933\n",
            "Epoch 00011: loss improved from 3.82918 to 3.59333, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 30s 18ms/step - loss: 3.5933\n",
            "Epoch 12/70\n",
            "1702/1703 [============================>.] - ETA: 0s - loss: 3.3549\n",
            "Epoch 00012: loss improved from 3.59333 to 3.35487, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 30s 18ms/step - loss: 3.3549\n",
            "Epoch 13/70\n",
            "1702/1703 [============================>.] - ETA: 0s - loss: 3.1229\n",
            "Epoch 00013: loss improved from 3.35487 to 3.12290, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 30s 17ms/step - loss: 3.1229\n",
            "Epoch 14/70\n",
            "1700/1703 [============================>.] - ETA: 0s - loss: 2.8891\n",
            "Epoch 00014: loss improved from 3.12290 to 2.88920, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 30s 17ms/step - loss: 2.8892\n",
            "Epoch 15/70\n",
            "1701/1703 [============================>.] - ETA: 0s - loss: 2.6570\n",
            "Epoch 00015: loss improved from 2.88920 to 2.65717, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 30s 18ms/step - loss: 2.6572\n",
            "Epoch 16/70\n",
            "1703/1703 [==============================] - ETA: 0s - loss: 2.4288\n",
            "Epoch 00016: loss improved from 2.65717 to 2.42879, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 29s 17ms/step - loss: 2.4288\n",
            "Epoch 17/70\n",
            "1702/1703 [============================>.] - ETA: 0s - loss: 2.2058\n",
            "Epoch 00017: loss improved from 2.42879 to 2.20593, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 30s 18ms/step - loss: 2.2059\n",
            "Epoch 18/70\n",
            "1701/1703 [============================>.] - ETA: 0s - loss: 1.9867\n",
            "Epoch 00018: loss improved from 2.20593 to 1.98692, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 30s 17ms/step - loss: 1.9869\n",
            "Epoch 19/70\n",
            "1702/1703 [============================>.] - ETA: 0s - loss: 1.7894\n",
            "Epoch 00019: loss improved from 1.98692 to 1.78947, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 29s 17ms/step - loss: 1.7895\n",
            "Epoch 20/70\n",
            "1703/1703 [==============================] - ETA: 0s - loss: 1.5987\n",
            "Epoch 00020: loss improved from 1.78947 to 1.59867, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 30s 17ms/step - loss: 1.5987\n",
            "Epoch 21/70\n",
            "1700/1703 [============================>.] - ETA: 0s - loss: 1.4331\n",
            "Epoch 00021: loss improved from 1.59867 to 1.43306, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 30s 17ms/step - loss: 1.4331\n",
            "Epoch 22/70\n",
            "1703/1703 [==============================] - ETA: 0s - loss: 1.2840\n",
            "Epoch 00022: loss improved from 1.43306 to 1.28404, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 30s 18ms/step - loss: 1.2840\n",
            "Epoch 23/70\n",
            "1703/1703 [==============================] - ETA: 0s - loss: 1.1627\n",
            "Epoch 00023: loss improved from 1.28404 to 1.16270, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 30s 18ms/step - loss: 1.1627\n",
            "Epoch 24/70\n",
            "1702/1703 [============================>.] - ETA: 0s - loss: 1.0602\n",
            "Epoch 00024: loss improved from 1.16270 to 1.06022, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 30s 17ms/step - loss: 1.0602\n",
            "Epoch 25/70\n",
            "1701/1703 [============================>.] - ETA: 0s - loss: 0.9660\n",
            "Epoch 00025: loss improved from 1.06022 to 0.96606, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 30s 17ms/step - loss: 0.9661\n",
            "Epoch 26/70\n",
            "1701/1703 [============================>.] - ETA: 0s - loss: 0.9031\n",
            "Epoch 00026: loss improved from 0.96606 to 0.90340, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 30s 17ms/step - loss: 0.9034\n",
            "Epoch 27/70\n",
            "1702/1703 [============================>.] - ETA: 0s - loss: 0.8386\n",
            "Epoch 00027: loss improved from 0.90340 to 0.83857, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 30s 18ms/step - loss: 0.8386\n",
            "Epoch 28/70\n",
            "1700/1703 [============================>.] - ETA: 0s - loss: 0.7914\n",
            "Epoch 00028: loss improved from 0.83857 to 0.79169, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 30s 17ms/step - loss: 0.7917\n",
            "Epoch 29/70\n",
            "1702/1703 [============================>.] - ETA: 0s - loss: 0.7537\n",
            "Epoch 00029: loss improved from 0.79169 to 0.75364, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 30s 17ms/step - loss: 0.7536\n",
            "Epoch 30/70\n",
            "1702/1703 [============================>.] - ETA: 0s - loss: 0.7153\n",
            "Epoch 00030: loss improved from 0.75364 to 0.71541, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 30s 17ms/step - loss: 0.7154\n",
            "Epoch 31/70\n",
            "1703/1703 [==============================] - ETA: 0s - loss: 0.6904\n",
            "Epoch 00031: loss improved from 0.71541 to 0.69041, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 30s 17ms/step - loss: 0.6904\n",
            "Epoch 32/70\n",
            "1701/1703 [============================>.] - ETA: 0s - loss: 0.6618\n",
            "Epoch 00032: loss improved from 0.69041 to 0.66187, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 30s 17ms/step - loss: 0.6619\n",
            "Epoch 33/70\n",
            "1702/1703 [============================>.] - ETA: 0s - loss: 0.6443\n",
            "Epoch 00033: loss improved from 0.66187 to 0.64436, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 29s 17ms/step - loss: 0.6444\n",
            "Epoch 34/70\n",
            "1700/1703 [============================>.] - ETA: 0s - loss: 0.6205\n",
            "Epoch 00034: loss improved from 0.64436 to 0.62090, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 30s 17ms/step - loss: 0.6209\n",
            "Epoch 35/70\n",
            "1701/1703 [============================>.] - ETA: 0s - loss: 0.6048\n",
            "Epoch 00035: loss improved from 0.62090 to 0.60500, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 30s 17ms/step - loss: 0.6050\n",
            "Epoch 36/70\n",
            "1701/1703 [============================>.] - ETA: 0s - loss: 0.5900\n",
            "Epoch 00036: loss improved from 0.60500 to 0.59006, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 30s 17ms/step - loss: 0.5901\n",
            "Epoch 37/70\n",
            "1703/1703 [==============================] - ETA: 0s - loss: 0.5777\n",
            "Epoch 00037: loss improved from 0.59006 to 0.57772, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 29s 17ms/step - loss: 0.5777\n",
            "Epoch 38/70\n",
            "1701/1703 [============================>.] - ETA: 0s - loss: 0.5698\n",
            "Epoch 00038: loss improved from 0.57772 to 0.56976, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 30s 17ms/step - loss: 0.5698\n",
            "Epoch 39/70\n",
            "1703/1703 [==============================] - ETA: 0s - loss: 0.5532\n",
            "Epoch 00039: loss improved from 0.56976 to 0.55324, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 30s 17ms/step - loss: 0.5532\n",
            "Epoch 40/70\n",
            "1701/1703 [============================>.] - ETA: 0s - loss: 0.5432\n",
            "Epoch 00040: loss improved from 0.55324 to 0.54317, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 30s 18ms/step - loss: 0.5432\n",
            "Epoch 41/70\n",
            "1703/1703 [==============================] - ETA: 0s - loss: 0.5335\n",
            "Epoch 00041: loss improved from 0.54317 to 0.53350, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 30s 18ms/step - loss: 0.5335\n",
            "Epoch 42/70\n",
            "1702/1703 [============================>.] - ETA: 0s - loss: 0.5251\n",
            "Epoch 00042: loss improved from 0.53350 to 0.52507, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 29s 17ms/step - loss: 0.5251\n",
            "Epoch 43/70\n",
            "1701/1703 [============================>.] - ETA: 0s - loss: 0.5201\n",
            "Epoch 00043: loss improved from 0.52507 to 0.52015, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 30s 17ms/step - loss: 0.5201\n",
            "Epoch 44/70\n",
            "1700/1703 [============================>.] - ETA: 0s - loss: 0.5088\n",
            "Epoch 00044: loss improved from 0.52015 to 0.50899, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 30s 18ms/step - loss: 0.5090\n",
            "Epoch 45/70\n",
            "1702/1703 [============================>.] - ETA: 0s - loss: 0.5026\n",
            "Epoch 00045: loss improved from 0.50899 to 0.50258, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 30s 18ms/step - loss: 0.5026\n",
            "Epoch 46/70\n",
            "1703/1703 [==============================] - ETA: 0s - loss: 0.4974\n",
            "Epoch 00046: loss improved from 0.50258 to 0.49741, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 29s 17ms/step - loss: 0.4974\n",
            "Epoch 47/70\n",
            "1703/1703 [==============================] - ETA: 0s - loss: 0.4900\n",
            "Epoch 00047: loss improved from 0.49741 to 0.49000, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 30s 18ms/step - loss: 0.4900\n",
            "Epoch 48/70\n",
            "1702/1703 [============================>.] - ETA: 0s - loss: 0.4812\n",
            "Epoch 00048: loss improved from 0.49000 to 0.48117, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 30s 18ms/step - loss: 0.4812\n",
            "Epoch 49/70\n",
            "1703/1703 [==============================] - ETA: 0s - loss: 0.4762\n",
            "Epoch 00049: loss improved from 0.48117 to 0.47624, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 31s 18ms/step - loss: 0.4762\n",
            "Epoch 50/70\n",
            "1701/1703 [============================>.] - ETA: 0s - loss: 0.4723\n",
            "Epoch 00050: loss improved from 0.47624 to 0.47232, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 30s 17ms/step - loss: 0.4723\n",
            "Epoch 51/70\n",
            "1700/1703 [============================>.] - ETA: 0s - loss: 0.4718\n",
            "Epoch 00051: loss improved from 0.47232 to 0.47199, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 30s 18ms/step - loss: 0.4720\n",
            "Epoch 52/70\n",
            "1703/1703 [==============================] - ETA: 0s - loss: 0.4612\n",
            "Epoch 00052: loss improved from 0.47199 to 0.46120, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 30s 18ms/step - loss: 0.4612\n",
            "Epoch 53/70\n",
            "1702/1703 [============================>.] - ETA: 0s - loss: 0.4553\n",
            "Epoch 00053: loss improved from 0.46120 to 0.45543, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 30s 18ms/step - loss: 0.4554\n",
            "Epoch 54/70\n",
            "1700/1703 [============================>.] - ETA: 0s - loss: 0.4509\n",
            "Epoch 00054: loss improved from 0.45543 to 0.45092, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 30s 18ms/step - loss: 0.4509\n",
            "Epoch 55/70\n",
            "1702/1703 [============================>.] - ETA: 0s - loss: 0.4506\n",
            "Epoch 00055: loss improved from 0.45092 to 0.45053, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 30s 18ms/step - loss: 0.4505\n",
            "Epoch 56/70\n",
            "1702/1703 [============================>.] - ETA: 0s - loss: 0.4456\n",
            "Epoch 00056: loss improved from 0.45053 to 0.44561, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 30s 18ms/step - loss: 0.4456\n",
            "Epoch 57/70\n",
            "1700/1703 [============================>.] - ETA: 0s - loss: 0.4381\n",
            "Epoch 00057: loss improved from 0.44561 to 0.43830, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 30s 18ms/step - loss: 0.4383\n",
            "Epoch 58/70\n",
            "1701/1703 [============================>.] - ETA: 0s - loss: 0.4389\n",
            "Epoch 00058: loss did not improve from 0.43830\n",
            "1703/1703 [==============================] - 29s 17ms/step - loss: 0.4390\n",
            "Epoch 59/70\n",
            "1700/1703 [============================>.] - ETA: 0s - loss: 0.4344\n",
            "Epoch 00059: loss improved from 0.43830 to 0.43461, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 30s 18ms/step - loss: 0.4346\n",
            "Epoch 60/70\n",
            "1702/1703 [============================>.] - ETA: 0s - loss: 0.4330\n",
            "Epoch 00060: loss improved from 0.43461 to 0.43318, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 30s 18ms/step - loss: 0.4332\n",
            "Epoch 61/70\n",
            "1703/1703 [==============================] - ETA: 0s - loss: 0.4271\n",
            "Epoch 00061: loss improved from 0.43318 to 0.42709, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 30s 18ms/step - loss: 0.4271\n",
            "Epoch 62/70\n",
            "1702/1703 [============================>.] - ETA: 0s - loss: 0.4232\n",
            "Epoch 00062: loss improved from 0.42709 to 0.42319, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 30s 18ms/step - loss: 0.4232\n",
            "Epoch 63/70\n",
            "1701/1703 [============================>.] - ETA: 0s - loss: 0.4203\n",
            "Epoch 00063: loss improved from 0.42319 to 0.42049, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 31s 18ms/step - loss: 0.4205\n",
            "Epoch 64/70\n",
            "1702/1703 [============================>.] - ETA: 0s - loss: 0.4177\n",
            "Epoch 00064: loss improved from 0.42049 to 0.41767, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 30s 18ms/step - loss: 0.4177\n",
            "Epoch 65/70\n",
            "1701/1703 [============================>.] - ETA: 0s - loss: 0.4193\n",
            "Epoch 00065: loss did not improve from 0.41767\n",
            "1703/1703 [==============================] - 29s 17ms/step - loss: 0.4193\n",
            "Epoch 66/70\n",
            "1702/1703 [============================>.] - ETA: 0s - loss: 0.4162\n",
            "Epoch 00066: loss improved from 0.41767 to 0.41614, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 31s 18ms/step - loss: 0.4161\n",
            "Epoch 67/70\n",
            "1702/1703 [============================>.] - ETA: 0s - loss: 0.4083\n",
            "Epoch 00067: loss improved from 0.41614 to 0.40836, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 30s 18ms/step - loss: 0.4084\n",
            "Epoch 68/70\n",
            "1701/1703 [============================>.] - ETA: 0s - loss: 0.4066\n",
            "Epoch 00068: loss improved from 0.40836 to 0.40673, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 31s 18ms/step - loss: 0.4067\n",
            "Epoch 69/70\n",
            "1701/1703 [============================>.] - ETA: 0s - loss: 0.4083\n",
            "Epoch 00069: loss did not improve from 0.40673\n",
            "1703/1703 [==============================] - 30s 18ms/step - loss: 0.4083\n",
            "Epoch 70/70\n",
            "1703/1703 [==============================] - ETA: 0s - loss: 0.4016\n",
            "Epoch 00070: loss improved from 0.40673 to 0.40160, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 31s 18ms/step - loss: 0.4016\n",
            "Enter your line: i am a\n",
            "['i', 'am', 'a']\n",
            "widower\n",
            "Enter your line: you are a\n",
            "['you', 'are', 'a']\n",
            "benefactor\n",
            "Enter your line: hai i am\n",
            "['hai', 'i', 'am']\n",
            "Error occurred:  in user code:\n",
            "\n",
            "    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1621, in predict_function  *\n",
            "        return step_function(self, iterator)\n",
            "    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1611, in step_function  **\n",
            "        outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
            "    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1604, in run_step  **\n",
            "        outputs = model.predict_step(data)\n",
            "    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1572, in predict_step\n",
            "        return self(x, training=False)\n",
            "    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n",
            "        raise e.with_traceback(filtered_tb) from None\n",
            "    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/input_spec.py\", line 263, in assert_input_compatibility\n",
            "        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n",
            "\n",
            "    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 3), found shape=(None, 2)\n",
            "\n",
            "Enter your line: this is a\n",
            "['this', 'is', 'a']\n",
            "map\n",
            "Enter your line: i want to\n",
            "['i', 'want', 'to']\n",
            "find\n",
            "Enter your line: will tell you\n",
            "['will', 'tell', 'you']\n",
            "it\n",
            "Enter your line: about is what\n",
            "['about', 'is', 'what']\n",
            "they\n",
            "Enter your line: they have been\n",
            "['they', 'have', 'been']\n",
            "turning\n",
            "Enter your line: my mind is filled\n",
            "['mind', 'is', 'filled']\n",
            "with\n",
            "Enter your line: thoughts are just\n",
            "['thoughts', 'are', 'just']\n",
            "over\n",
            "Enter your line: kiana is sleeping please do\n",
            "['sleeping', 'please', 'do']\n",
            "you\n",
            "Enter your line: do you want\n",
            "['do', 'you', 'want']\n",
            "he\n",
            "Enter your line: he is a little\n",
            "['is', 'a', 'little']\n",
            "reward\n",
            "Enter your line: my sly fox\n",
            "['my', 'sly', 'fox']\n",
            "Error occurred:  in user code:\n",
            "\n",
            "    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1621, in predict_function  *\n",
            "        return step_function(self, iterator)\n",
            "    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1611, in step_function  **\n",
            "        outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
            "    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1604, in run_step  **\n",
            "        outputs = model.predict_step(data)\n",
            "    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1572, in predict_step\n",
            "        return self(x, training=False)\n",
            "    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n",
            "        raise e.with_traceback(filtered_tb) from None\n",
            "    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/input_spec.py\", line 263, in assert_input_compatibility\n",
            "        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n",
            "\n",
            "    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 3), found shape=(None, 2)\n",
            "\n",
            "Enter your line: i am kind\n",
            "['i', 'am', 'kind']\n",
            "of\n",
            "Enter your line: looking for you\n",
            "['looking', 'for', 'you']\n",
            "could\n",
            "Enter your line: could you lend\n",
            "['could', 'you', 'lend']\n",
            "Error occurred:  in user code:\n",
            "\n",
            "    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1621, in predict_function  *\n",
            "        return step_function(self, iterator)\n",
            "    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1611, in step_function  **\n",
            "        outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
            "    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1604, in run_step  **\n",
            "        outputs = model.predict_step(data)\n",
            "    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1572, in predict_step\n",
            "        return self(x, training=False)\n",
            "    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n",
            "        raise e.with_traceback(filtered_tb) from None\n",
            "    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/input_spec.py\", line 263, in assert_input_compatibility\n",
            "        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n",
            "\n",
            "    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 3), found shape=(None, 2)\n",
            "\n",
            "Enter your line: 0\n",
            "Execution completed.....\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "file = open(\"1661-0.txt\", \"r\", encoding = \"utf8\")\n",
        "\n",
        "# store file in list\n",
        "lines = []\n",
        "for i in file:\n",
        "    lines.append(i)\n",
        "\n",
        "# Convert list to string\n",
        "data = \"\"\n",
        "for i in lines:\n",
        "  data = ' '. join(lines) \n",
        "\n",
        "#replace unnecessary stuff with space\n",
        "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '').replace('“','').replace('”','')  #new line, carriage return, unicode character --> replace by space\n",
        "\n",
        "#remove unnecessary spaces \n",
        "data = data.split()\n",
        "data = ' '.join(data)\n",
        "data[:500]\n",
        "\n",
        "len(data)\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([data])\n",
        "\n",
        "# saving the tokenizer for predict function\n",
        "pickle.dump(tokenizer, open('token.pkl', 'wb'))\n",
        "\n",
        "sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
        "sequence_data[:15]\n",
        "\n",
        "len(sequence_data)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(vocab_size)\n",
        "\n",
        "sequences = []\n",
        "\n",
        "for i in range(3, len(sequence_data)):\n",
        "    words = sequence_data[i-3:i+1]\n",
        "    sequences.append(words)\n",
        "    \n",
        "print(\"The Length of sequences are: \", len(sequences))\n",
        "sequences = np.array(sequences)\n",
        "sequences[:10]\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for i in sequences:\n",
        "    X.append(i[0:3])\n",
        "    y.append(i[3])\n",
        "    \n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "print(\"Data: \", X[:10])\n",
        "print(\"Response: \", y[:10])\n",
        "\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "y[:5]\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 10, input_length=3))\n",
        "model.add(LSTM(1000, return_sequences=True))\n",
        "model.add(LSTM(1000))\n",
        "model.add(Dense(1000, activation=\"relu\"))\n",
        "model.add(Dense(vocab_size, activation=\"softmax\"))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "keras.utils.plot_model(model, to_file='plot.png', show_layer_names=True)\n",
        "\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "checkpoint = ModelCheckpoint(\"next_words.h5\", monitor='loss', verbose=1, save_best_only=True)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=0.001))\n",
        "model.fit(X, y, epochs=70, batch_size=64, callbacks=[checkpoint])\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model = load_model('next_words.h5')\n",
        "tokenizer = pickle.load(open('token.pkl', 'rb'))\n",
        "\n",
        "def Predict_Next_Words(model, tokenizer, text):\n",
        "\n",
        "  sequence = tokenizer.texts_to_sequences([text])\n",
        "  sequence = np.array(sequence)\n",
        "  preds = np.argmax(model.predict(sequence))\n",
        "  predicted_word = \"\"\n",
        "  \n",
        "  for key, value in tokenizer.word_index.items():\n",
        "      if value == preds:\n",
        "          predicted_word = key\n",
        "          break\n",
        "  \n",
        "  print(predicted_word)\n",
        "  return predicted_word\n",
        "\n",
        "while(True):\n",
        "  text = input(\"Enter your line: \")\n",
        "  \n",
        "  if text == \"0\":\n",
        "      print(\"Execution completed.....\")\n",
        "      break\n",
        "  \n",
        "  else:\n",
        "      try:\n",
        "          text = text.split(\" \")\n",
        "          text = text[-3:]\n",
        "          print(text)\n",
        "        \n",
        "          Predict_Next_Words(model, tokenizer, text)\n",
        "          \n",
        "      except Exception as e:\n",
        "        print(\"Error occurred: \",e)\n",
        "        continue"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}